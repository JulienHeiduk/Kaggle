{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "c61d38c6-ff97-2f23-2c05-09e683020f93"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression, ElasticNet\n",
    "\n",
    "df = pd.read_csv(\"../input/train.csv\")\n",
    "df_test = pd.read_csv(\"../input/test.csv\")\n",
    "y = df[\"type\"]\n",
    "y_train_nn = pd.get_dummies(df[[\"type\"]], prefix=\"\")\n",
    "df = df.drop(\"type\",axis=1)\n",
    "#df.drop(\"color\", inplace=True, axis=1)\n",
    "#df_test.drop(\"color\", inplace=True, axis=1)\n",
    "df = pd.get_dummies(df)\n",
    "df_test = pd.get_dummies(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "0bb10969-7dcf-2620-6967-5f96207b454c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n"
     ]
    }
   ],
   "source": [
    "df['bone_rotting'] = df['bone_length']*df['rotting_flesh']\n",
    "df['bone_soul'] = df['bone_length']*df['has_soul']\n",
    "df['bone_hair'] = df['bone_length']*df['hair_length']\n",
    "df['flesh_hair'] = df['rotting_flesh']*df['hair_length']\n",
    "df['flesh_soul'] = df['rotting_flesh']*df['has_soul']\n",
    "\n",
    "\n",
    "df_test['bone_rotting'] = df_test['bone_length']*df_test['rotting_flesh']\n",
    "df_test['bone_soul'] = df_test['bone_length']*df_test['has_soul']\n",
    "df_test['bone_hair'] = df_test['bone_length']*df_test['hair_length']\n",
    "df_test['flesh_hair'] = df_test['rotting_flesh']*df_test['hair_length']\n",
    "df_test['flesh_soul'] = df_test['rotting_flesh']*df_test['has_soul']\n",
    "\n",
    "#df = df[[\"bone_length\",\"rotting_flesh\",\"hair_length\",\"color\",\"has_soul\"]]\n",
    "#df_test = [[\"bone_length\",\"rotting_flesh\",\"hair_length\",\"color\",\"has_soul\"]]\n",
    "#df_test.fillna(0,inplace=True)\n",
    "#df.fillna(0,inplace=True)\n",
    "\n",
    "df_test.replace([np.inf, -np.inf], 100,inplace=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=0)\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=750,max_depth=7,n_jobs=-1)\n",
    "rfc.fit(X_train,y_train)\n",
    "y_pred = rfc.predict(X_test) \n",
    "\n",
    "print(accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "03bc7460-ee97-5322-eb04-67077f9a3e73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.306666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "NN = MLPClassifier(solver='lbfgs',hidden_layer_sizes=(15,3),verbose=True,random_state =42)\n",
    "NN.fit(X_train,y_train)\n",
    "y_pred = NN.predict(X_test) \n",
    "\n",
    "print(accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "061696de-b555-580f-7a98-3b48f417b291"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Change warning: `feature_columns` will be required after 2016-08-01.\n",
      "Instructions for updating:\n",
      "Pass `tf.contrib.learn.infer_real_valued_columns_from_input(x)` or `tf.contrib.learn.infer_real_valued_columns_from_input_fn(input_fn)` as `feature_columns`, where `x` or `input_fn` is your argument to `fit`, `evaluate`, or `predict`.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpmsm3qydi\n",
      "WARNING:tensorflow:Setting feature info to TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(None), Dimension(16)]), is_sparse=False)\n",
      "WARNING:tensorflow:Setting targets info to TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(None)]), is_sparse=False)\n",
      "INFO:tensorflow:Transforming feature_column _RealValuedColumn(column_name='', dimension=16, default_value=None, dtype=tf.float32)\n",
      "INFO:tensorflow:Create CheckpointSaver\n",
      "INFO:tensorflow:Step 1: loss = 64.5151\n",
      "INFO:tensorflow:Step 101: loss = 1.17137\n",
      "INFO:tensorflow:Step 201: loss = 1.5902\n",
      "INFO:tensorflow:Saving checkpoints for 300 into /tmp/tmpmsm3qydi/model.ckpt.\n",
      "INFO:tensorflow:Step 301: loss = 1.6059\n",
      "INFO:tensorflow:Step 401: loss = 1.27795\n",
      "INFO:tensorflow:Step 501: loss = 1.22352\n",
      "INFO:tensorflow:Saving checkpoints for 600 into /tmp/tmpmsm3qydi/model.ckpt.\n",
      "INFO:tensorflow:Step 601: loss = 1.18336\n",
      "INFO:tensorflow:Step 701: loss = 1.139\n",
      "INFO:tensorflow:Step 801: loss = 1.08142\n",
      "INFO:tensorflow:Saving checkpoints for 900 into /tmp/tmpmsm3qydi/model.ckpt.\n",
      "INFO:tensorflow:Step 901: loss = 1.05722\n",
      "INFO:tensorflow:Step 1001: loss = 1.025\n",
      "INFO:tensorflow:Step 1101: loss = 0.992533\n",
      "INFO:tensorflow:Saving checkpoints for 1200 into /tmp/tmpmsm3qydi/model.ckpt.\n",
      "INFO:tensorflow:Step 1201: loss = 0.979123\n",
      "INFO:tensorflow:Step 1301: loss = 0.959416\n",
      "INFO:tensorflow:Step 1401: loss = 0.940723\n",
      "INFO:tensorflow:Saving checkpoints for 1500 into /tmp/tmpmsm3qydi/model.ckpt.\n",
      "INFO:tensorflow:Step 1501: loss = 0.923799\n",
      "INFO:tensorflow:Step 1601: loss = 0.909268\n",
      "INFO:tensorflow:Step 1701: loss = 0.89561\n",
      "INFO:tensorflow:Saving checkpoints for 1800 into /tmp/tmpmsm3qydi/model.ckpt.\n",
      "INFO:tensorflow:Step 1801: loss = 0.883313\n",
      "INFO:tensorflow:Step 1901: loss = 0.872012\n",
      "INFO:tensorflow:Step 2001: loss = 0.861731\n",
      "INFO:tensorflow:Saving checkpoints for 2100 into /tmp/tmpmsm3qydi/model.ckpt.\n",
      "INFO:tensorflow:Step 2101: loss = 0.852049\n",
      "INFO:tensorflow:Step 2201: loss = 0.843047\n",
      "INFO:tensorflow:Step 2301: loss = 0.833667\n",
      "INFO:tensorflow:Saving checkpoints for 2400 into /tmp/tmpmsm3qydi/model.ckpt.\n",
      "INFO:tensorflow:Step 2401: loss = 0.826398\n",
      "INFO:tensorflow:Step 2501: loss = 0.817847\n",
      "INFO:tensorflow:Step 2601: loss = 0.811611\n",
      "INFO:tensorflow:Saving checkpoints for 2700 into /tmp/tmpmsm3qydi/model.ckpt.\n",
      "INFO:tensorflow:Step 2701: loss = 0.804974\n",
      "INFO:tensorflow:Step 2801: loss = 0.798565\n",
      "INFO:tensorflow:Step 2901: loss = 0.792227\n",
      "INFO:tensorflow:Saving checkpoints for 3000 into /tmp/tmpmsm3qydi/model.ckpt.\n",
      "INFO:tensorflow:Step 3001: loss = 0.785795\n",
      "INFO:tensorflow:Step 3101: loss = 0.779632\n",
      "INFO:tensorflow:Step 3201: loss = 0.77362\n",
      "INFO:tensorflow:Saving checkpoints for 3300 into /tmp/tmpmsm3qydi/model.ckpt.\n",
      "INFO:tensorflow:Step 3301: loss = 0.767992\n",
      "INFO:tensorflow:Step 3401: loss = 0.762763\n",
      "INFO:tensorflow:Step 3501: loss = 0.757264\n",
      "INFO:tensorflow:Saving checkpoints for 3600 into /tmp/tmpmsm3qydi/model.ckpt.\n",
      "INFO:tensorflow:Step 3601: loss = 0.751679\n",
      "INFO:tensorflow:Step 3701: loss = 0.746594\n",
      "INFO:tensorflow:Step 3801: loss = 0.740928\n",
      "INFO:tensorflow:Saving checkpoints for 3900 into /tmp/tmpmsm3qydi/model.ckpt.\n",
      "INFO:tensorflow:Step 3901: loss = 0.734825\n",
      "INFO:tensorflow:Step 4001: loss = 0.729621\n",
      "INFO:tensorflow:Step 4101: loss = 0.725039\n",
      "INFO:tensorflow:Saving checkpoints for 4200 into /tmp/tmpmsm3qydi/model.ckpt.\n",
      "INFO:tensorflow:Step 4201: loss = 0.720781\n",
      "INFO:tensorflow:Step 4301: loss = 0.716642\n",
      "INFO:tensorflow:Step 4401: loss = 0.712633\n",
      "INFO:tensorflow:Saving checkpoints for 4500 into /tmp/tmpmsm3qydi/model.ckpt.\n",
      "INFO:tensorflow:Step 4501: loss = 0.708752\n",
      "INFO:tensorflow:Step 4601: loss = 0.705\n",
      "INFO:tensorflow:Step 4701: loss = 0.701316\n",
      "INFO:tensorflow:Saving checkpoints for 4800 into /tmp/tmpmsm3qydi/model.ckpt.\n",
      "INFO:tensorflow:Step 4801: loss = 0.697788\n",
      "INFO:tensorflow:Step 4901: loss = 0.694303\n",
      "INFO:tensorflow:Step 5001: loss = 0.690918\n",
      "INFO:tensorflow:Saving checkpoints for 5100 into /tmp/tmpmsm3qydi/model.ckpt.\n",
      "INFO:tensorflow:Step 5101: loss = 0.687621\n",
      "INFO:tensorflow:Step 5201: loss = 0.684463\n",
      "INFO:tensorflow:Step 5301: loss = 0.681269\n",
      "INFO:tensorflow:Saving checkpoints for 5400 into /tmp/tmpmsm3qydi/model.ckpt.\n",
      "INFO:tensorflow:Step 5401: loss = 0.67824\n",
      "INFO:tensorflow:Saving checkpoints for 5500 into /tmp/tmpmsm3qydi/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.675306.\n",
      "INFO:tensorflow:Transforming feature_column _RealValuedColumn(column_name='', dimension=16, default_value=None, dtype=tf.float32)\n",
      "INFO:tensorflow:Loading model from checkpoint: /tmp/tmpmsm3qydi/model.ckpt-5500-?????-of-00001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.726351351351\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import sklearn.ensemble as ske\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder as LE\n",
    "\n",
    "letype = LE()\n",
    "y_NN = letype.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y_NN, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "classifier = learn.DNNClassifier(hidden_units=[15], n_classes=3)\n",
    "classifier.fit(x=X_train, y=y_train, max_steps=5500)\n",
    "predictions = classifier.predict(X_train)\n",
    "\n",
    "print(accuracy_score(y_train, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "8b2f2b9e-0880-99c8-7bea-6481efff35ee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Transforming feature_column _RealValuedColumn(column_name='', dimension=16, default_value=None, dtype=tf.float32)\n",
      "INFO:tensorflow:Loading model from checkpoint: /tmp/tmpmsm3qydi/model.ckpt-5500-?????-of-00001.\n"
     ]
    }
   ],
   "source": [
    "X_test = df_test\n",
    "pred = classifier.predict(X_test)\n",
    "pred = letype.inverse_transform(pred)\n",
    "pred\n",
    "output = pd.DataFrame({\"id\": df_test[\"id\"],\"type\":pred})\n",
    "output.to_csv('DNN_Classifier.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "7171b8a3-057c-dfc7-aabc-994c088547c3"
   },
   "outputs": [],
   "source": [
    "## A dead simple neural network class in Python+Numpy. Plain SGD, and no regularization.\n",
    "def sigmoid(X):\n",
    "    return 1.0 / ( 1.0 + np.exp(-X) )\n",
    "\n",
    "def softmax(X):\n",
    "    _sum = np.exp(X).sum()\n",
    "    return np.exp(X) / _sum\n",
    "\n",
    "class neuralnet(object):\n",
    "    def __init__(self, num_input, num_hidden, num_output):\n",
    "        self._W1 = (np.random.random_sample((num_input, num_hidden)) - 0.5).astype(np.float32)\n",
    "        self._b1 = np.zeros((1, num_hidden)).astype(np.float32)\n",
    "        self._W2 = (np.random.random_sample((num_hidden, num_output)) - 0.5).astype(np.float32)\n",
    "        self._b2 = np.zeros((1, num_output)).astype(np.float32)\n",
    "\n",
    "    def forward(self,X):\n",
    "        net1 = np.matmul( X, self._W1 ) + self._b1\n",
    "        y = sigmoid(net1)\n",
    "        net2 = np.matmul( y, self._W2 ) + self._b2\n",
    "        z = softmax(net2)\n",
    "        return z,y\n",
    "\n",
    "    def backpropagation(self, X, target, eta):\n",
    "        z, y = self.forward(X)\n",
    "        d2 = (z - target)\n",
    "        d1 = y*(1.0-y) * np.matmul(d2, self._W2.T)\n",
    "        # The updates are done within this method. This more or less implies\n",
    "        # utpdates with Stochastic Gradient Decent. Let's fix that later.\n",
    "        # TODO: Support for full batch and mini-batches etc.\n",
    "        self._W2 -= eta * np.matmul(y.T,d2)\n",
    "        self._W1 -= eta * np.matmul(X.reshape((-1,1)),d1)\n",
    "        self._b2 -= eta * d2\n",
    "        self._b1 -= eta * d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "635cfcfd-9cf4-12a9-d3cc-5b0dda5f8dba"
   },
   "outputs": [],
   "source": [
    "# Some hyper-parameters to tune.\n",
    "num_hidden = 12\n",
    "n_epochs   = 1500\n",
    "eta        = 0.01\n",
    "nn = neuralnet( df.shape[1], num_hidden,  y_train_nn.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "f5791349-43b5-a2c5-0ab8-c261f09977bf"
   },
   "outputs": [],
   "source": [
    "# (EDIT: It's much faster to convert the dataframes to numpy arrays and then iterate)\n",
    "X = np.array(df, dtype=np.float32)\n",
    "Y = np.array(y_train_nn, dtype=np.float32)\n",
    "for epoch in range(n_epochs):\n",
    "    for monster, target in zip(X,Y):\n",
    "        nn.backpropagation( monster, target, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "8ed495a4-d8bd-65b5-5a56-25fcb23b5a02"
   },
   "outputs": [],
   "source": [
    "with open('submission-{}-hidden.csv'.format(num_hidden), 'w') as f:\n",
    "    f.write(\"id,type\\n\")\n",
    "    for index, monster in df_test.iterrows():\n",
    "        probs = nn.forward( np.array(monster, dtype=np.float32))[0]\n",
    "        f.write(\"{},{}\\n\".format(df_test[\"id\"][index], y_train_nn.columns.values[np.argmax(probs)][1:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "33c35b34-7b21-6c19-7cdf-99239d7c3279"
   },
   "outputs": [],
   "source": [
    "def score(clf, train_np, random_state, folds, target, length):\n",
    "    kf = KFold(n = length , n_folds=folds, shuffle = False, random_state = random_state)\n",
    "    for itrain, itest in kf:\n",
    "        Xtr, Xte = train_np[itrain], train_np[itest]\n",
    "        ytr, yte = target[itrain], target[itest]\n",
    "        clf.fit(Xtr, ytr.ravel())\n",
    "        pred = pd.DataFrame(clf.predict(Xte))\n",
    "        return accuracy_score(yte, pred)\n",
    "    return accuracy_score(y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "2ae688a4-7cfb-2fac-266c-43374f3baf5c"
   },
   "outputs": [],
   "source": [
    "df_array = np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "012ff719-a12a-4ef0-9296-aa82299aaa66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/ipykernel/__main__.py:71: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/opt/conda/lib/python3.5/site-packages/ipykernel/__main__.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/opt/conda/lib/python3.5/site-packages/ipykernel/__main__.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n = 371 , n_folds=3, shuffle = False, random_state = 42)\n",
    "clf = ExtraTreesClassifier(n_estimators=750,max_depth=7,n_jobs=-1,random_state=42)\n",
    "#clf = LogisticRegression(penalty='l2',C=1000000)\n",
    "#clf = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(100,), random_state=1)\n",
    "\n",
    "i = 0\n",
    "for itrain, itest in kf:\n",
    "    print(i)\n",
    "    Xtr, Xte = df_array[itrain], df_array[itest]\n",
    "    ytr, yte = y[itrain], y[itest]\n",
    "    clf.fit(Xtr, ytr.ravel())\n",
    "    if i == 0:\n",
    "        pred_0 = pd.DataFrame(clf.predict(df_test))\n",
    "        pred_0.columns = ['0']\n",
    "    if i == 1:\n",
    "        pred_1 = pd.DataFrame(clf.predict(df_test))\n",
    "        pred_1.columns = ['1']\n",
    "    if i == 2:\n",
    "        pred_2 = pd.DataFrame(clf.predict(df_test))  \n",
    "        pred_2.columns = ['2']\n",
    "    i = i + 1  \n",
    "\n",
    "clf.fit(df, y)\n",
    "pred_tot = clf.predict(df_test)\n",
    "Y = pd.DataFrame()\n",
    "Y[\"id\"] = df_test[\"id\"]\n",
    "Y[\"type\"] = pred_tot\n",
    "Y.to_csv(\"submission_total.csv\",index=False)\n",
    "\n",
    "Y = pd.DataFrame()\n",
    "Y[\"id\"] = df_test[\"id\"]\n",
    "Y[\"type\"] = pred_0\n",
    "Y.to_csv(\"submission0.csv\",index=False)\n",
    "\n",
    "Y = pd.DataFrame()\n",
    "Y[\"id\"] = df_test[\"id\"]\n",
    "Y[\"type\"] = pred_1\n",
    "Y.to_csv(\"submission1.csv\",index=False)\n",
    "\n",
    "Y = pd.DataFrame()\n",
    "Y[\"id\"] = df_test[\"id\"]\n",
    "Y[\"type\"] = pred_2\n",
    "Y.to_csv(\"submission2.csv\",index=False)\n",
    "\n",
    "result = pd.concat([pred_0, pred_1, pred_2], axis=1)\n",
    "\n",
    "result['Ghoul0']=(result['0']=='Ghoul').astype(int)\n",
    "result['Ghoul1']=(result['1']=='Ghoul').astype(int)\n",
    "result['Ghoul2']=(result['2']=='Ghoul').astype(int)\n",
    "\n",
    "result['Ghost0']=(result['0']=='Ghost').astype(int)\n",
    "result['Ghost1']=(result['1']=='Ghost').astype(int)\n",
    "result['Ghost2']=(result['2']=='Ghost').astype(int)\n",
    "\n",
    "result['Gobelin0']=(result['0']=='Goblin').astype(int)\n",
    "result['Gobelin1']=(result['1']=='Goblin').astype(int)\n",
    "result['Gobelin2']=(result['2']=='Goblin').astype(int)\n",
    "\n",
    "result['Ghoul_fin'] = result['Ghoul0'] + result['Ghoul1'] + result['Ghoul2']\n",
    "result['Ghost_fin'] = result['Ghost0'] + result['Ghost1'] + result['Ghost2']\n",
    "result['Gobelin_fin'] = result['Gobelin0'] + result['Gobelin1'] + result['Gobelin2']\n",
    "\n",
    "result['type'] = 'Ghoul'\n",
    "\n",
    "for i in range(1,529):\n",
    "    if result['Ghoul_fin'][i]>1:\n",
    "        result['type'][i] = 'Ghoul'\n",
    "    if result['Ghost_fin'][i]>1:\n",
    "        result['type'][i] = 'Ghost'\n",
    "    if result['Gobelin_fin'][i]>1:\n",
    "        result['type'][i] = 'Goblin'     \n",
    "\n",
    "Y = pd.DataFrame()\n",
    "Y[\"id\"] = df_test[\"id\"]\n",
    "Y[\"type\"] = result['type']\n",
    "Y.to_csv(\"submission_fin.csv\",index=False)        "
   ]
  }
 ],
 "metadata": {
  "_change_revision": 3,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
